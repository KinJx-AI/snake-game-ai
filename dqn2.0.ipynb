{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "379f6979-749e-4da8-9487-43254b4cedad",
   "metadata": {},
   "source": [
    "# Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e249390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "890e25b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdda74f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c71cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snake_model import *\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import random as rd\n",
    "\n",
    "from icecream import ic\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc41a3-db60-4040-9606-a19341eaaa4b",
   "metadata": {},
   "source": [
    "# Constants & Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ccf278-655d-4e25-a543-448e75bb5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.9\n",
    "EPSILON_DECAY_FACTOR = 0.999\n",
    "REPLAY_BUFFER_SIZE = 1000\n",
    "INIT_REPLAY_COUNT = REPLAY_BUFFER_SIZE // 2\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def log(text):\n",
    "    logging.basicConfig(filename=\"log.txt\", level=logging.DEBUG)\n",
    "    logging.debug(text)\n",
    "\n",
    "def head_to_one_hot(head, gridSize):\n",
    "    one_hot = np.zeros((gridSize, gridSize))\n",
    "    one_hot[head.x][head.y] = 1\n",
    "    return one_hot\n",
    "\n",
    "def body_to_one_hot(bodyBlocks, gridSize):\n",
    "    one_hot = np.zeros((gridSize, gridSize))\n",
    "    for bodyBlock in bodyBlocks:\n",
    "        one_hot[bodyBlock.x][bodyBlock.y] = 1\n",
    "    return one_hot\n",
    "\n",
    "def food_to_one_hot(foods, gridSize):\n",
    "    one_hot = np.zeros((gridSize, gridSize))\n",
    "    for food in foods:\n",
    "        one_hot[food.x][food.y] = 1\n",
    "    return one_hot\n",
    "\n",
    "def direction_to_one_hot(direction):\n",
    "    one_hot = np.zeros(4)\n",
    "    one_hot[direction] = 1\n",
    "    return one_hot\n",
    "\n",
    "def action_to_direction(currentDirection, chosenAction): # 0up 1down 2left 3right : 0left 1stay 2right\n",
    "    if currentDirection == 0:\n",
    "        if chosenAction == 0:\n",
    "            return 2\n",
    "        if chosenAction == 2:\n",
    "            return 3\n",
    "    if currentDirection == 1:\n",
    "        if chosenAction == 0:\n",
    "            return 3\n",
    "        if chosenAction == 2:\n",
    "            return 2\n",
    "    if currentDirection == 2:\n",
    "        if chosenAction == 0:\n",
    "            return 1\n",
    "        if chosenAction == 2:\n",
    "            return 0\n",
    "    if currentDirection == 3:\n",
    "        if chosenAction == 0:\n",
    "            return 0\n",
    "        if chosenAction == 2:\n",
    "            return 1\n",
    "    return currentDirection\n",
    "\n",
    "def get_projected_coodinates(x, y, direction):\n",
    "    if currentDirection == 0:\n",
    "        return (x-1, y)\n",
    "    if currentDirection == 1:\n",
    "        return (x+1, y)\n",
    "    if currentDirection == 2:\n",
    "        return (x, y-1)\n",
    "    if currentDirection == 3:\n",
    "        return (x, y+1)\n",
    "\n",
    "def get_mini_batch(replay):\n",
    "    mini_batch = rd.sample(replay, BATCH_SIZE) \n",
    "    col_indices = [0,1,2,3]\n",
    "    result = [list(column) for column in zip(*mini_batch)][col_indices[0]:col_indices[-1]+1]\n",
    "    return np.array(result[0]), np.array(result[1]), np.array(result[2]), np.array(result[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c83fd61",
   "metadata": {},
   "source": [
    "# Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98745dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.q_net = self.build_dqn_model(state_size, action_size)\n",
    "        self.target_q_net = self.build_dqn_model(state_size, action_size)\n",
    "\n",
    "    def build_dqn_model(self, state_size, action_size):\n",
    "\n",
    "        l1 = state_size\n",
    "        l2 = 128\n",
    "        l3 = 64\n",
    "        l4 = action_size\n",
    "        \n",
    "        q_net = tf.keras.Sequential()\n",
    "        q_net.add(layers.Dense(l2, input_dim=l1, activation='relu', kernel_initializer='he_uniform'))\n",
    "        q_net.add(layers.Dense(l3, activation='relu', kernel_initializer='he_uniform'))\n",
    "        q_net.add(layers.Dense(l4, activation='linear', kernel_initializer='he_uniform'))\n",
    "        q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
    "        return q_net\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_q_net.set_weights(self.q_net.get_weights())\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        state = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        q_values = self.q_net(state)\n",
    "        return q_values.numpy()\n",
    "\n",
    "    def train(self, batch):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch = batch\n",
    "        current_q = self.q_net(state_batch).numpy()\n",
    "        target_q = np.copy(current_q)\n",
    "        next_q = self.target_q_net(next_state_batch).numpy()\n",
    "        max_next_q = np.amax(next_q, axis=1)\n",
    "        for i in range(state_batch.shape[0]):\n",
    "            target_q_val = reward_batch[i]\n",
    "            target_q_val += 0.95 * max_next_q[i]\n",
    "            target_q[i][action_batch[i]] = target_q_val\n",
    "        training_history = self.q_net.fit(x=state_batch, y=target_q, verbose=0)\n",
    "        loss = training_history.history['loss']\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be56ae6e-2861-4f60-94b7-8017ebe27331",
   "metadata": {},
   "source": [
    "# Deep Q Learning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c478a5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.world = World()\n",
    "        self.state = self.get_state()\n",
    "        self.prevState = None\n",
    "        self.score = self.world.score\n",
    "        self.prevScore = self.score\n",
    "\n",
    "    def reset(self):\n",
    "        self.world.__init__()\n",
    "    \n",
    "    def get_state(self):\n",
    "        headArray = head_to_one_hot(self.world.snake.head, self.world.size).flatten()\n",
    "        bodyArray = body_to_one_hot(self.world.snake.body, self.world.size).flatten()\n",
    "        foodArray = food_to_one_hot(self.world.foods, self.world.size).flatten()\n",
    "        directionArray = direction_to_one_hot(self.world.snake.direction).flatten()\n",
    "        state = np.array([headArray, bodyArray, foodArray]).flatten()\n",
    "        state = np.append(state, directionArray)\n",
    "        return state\n",
    "\n",
    "    def step(self):\n",
    "        self.prevState = self.get_state()\n",
    "        self.prevScore = self.world.score\n",
    "        self.world.step()\n",
    "        if self.world.isCollide:\n",
    "            self.reset()\n",
    "        self.state = self.get_state()\n",
    "        self.score = self.world.score\n",
    "            \n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.environment = Environment()\n",
    "        self.network = DQN((self.environment.world.size**2) * 3 + 4, 3)\n",
    "        self.replay = []\n",
    "        self.epsilon = 1\n",
    "\n",
    "        self.highScore = 0\n",
    "\n",
    "    def remember(self, action):\n",
    "        prevState = self.environment.prevState\n",
    "        reward = self.get_reward()\n",
    "        newState = self.environment.state\n",
    "        self.replay.append([prevState, action, reward, newState])\n",
    "        if len(self.replay) > REPLAY_BUFFER_SIZE:\n",
    "            self.replay.pop()\n",
    "\n",
    "    def get_reward(self):\n",
    "        reward = -1\n",
    "        if self.environment.score > self.environment.prevScore:\n",
    "            reward = 100\n",
    "        if self.environment.score < self.environment.prevScore or self.environment.score == 0:\n",
    "            reward = -5\n",
    "        return reward\n",
    "        \n",
    "    def e_greedy(self, epsilon):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            chosenAction = np.random.choice([0,1,2])\n",
    "            return chosenAction\n",
    "        else:\n",
    "            currentState = self.environment.get_state()\n",
    "            currentQVals = self.network.get_qvals([currentState])\n",
    "            return np.argmax(currentQVals)\n",
    "\n",
    "    def train(self):\n",
    "        miniBatch = get_mini_batch(self.replay)\n",
    "        self.network.train(miniBatch)\n",
    "\n",
    "    def step(self):\n",
    "        chosenAction = self.e_greedy(self.epsilon)\n",
    "        newDirection = action_to_direction(self.environment.world.snake.direction, chosenAction)\n",
    "        self.environment.world.snake.change_direction(newDirection)\n",
    "        \n",
    "        self.environment.step()\n",
    "        \n",
    "        self.remember(chosenAction)\n",
    "        if self.environment.score > self.highScore:\n",
    "            self.highScore = self.environment.score      \n",
    "\n",
    "class DeepQLearning:\n",
    "    def __init__(self):\n",
    "        self.time = 0\n",
    "        self.agent = Agent()\n",
    "        self.init_replay()\n",
    "\n",
    "    def init_replay(self):\n",
    "        for i in range(INIT_REPLAY_COUNT):\n",
    "            self.agent.step()\n",
    "\n",
    "    def step(self):\n",
    "        self.agent.step()\n",
    "        self.time += 1\n",
    "\n",
    "        if self.time % 10 == 0:\n",
    "            self.agent.train()\n",
    "            self.agent.epsilon *= EPSILON_DECAY_FACTOR\n",
    "\n",
    "        if self.time % 1000 == 0:\n",
    "            self.agent.network.update_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "280e8deb-815a-4e42-b398-1e357d82a36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time :23020 | Epsilon : 0.09994334856146549 | High Score : 4 | Current Score : 1 |\r"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test = DeepQLearning()\n",
    "\n",
    "epsilon = test.agent.epsilon\n",
    "while epsilon >= 0.1:\n",
    "    test.step()\n",
    "    epsilon = test.agent.epsilon\n",
    "    print(f\"Time :{test.time} | Epsilon : {epsilon} | High Score : {test.agent.highScore} | Current Score : {test.agent.environment.world.score} |\", end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f9b43-2262-4b02-ac88-35c86f746038",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ac00776-530e-449b-8aef-00ac2ddd39f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.9.18)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from sys import exit\n",
    "from snake_view import *\n",
    "\n",
    "TICK_RATE = 30\n",
    "\n",
    "pygame.init()\n",
    "pygame.display.set_caption(\"Snake\")\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, driver):\n",
    "        self.dqnDriver = driver\n",
    "        \n",
    "        self.agent = self.dqnDriver.agent\n",
    "        self.gameWorld = self.agent.environment.world\n",
    "        self.UI = UI(self.gameWorld)\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def game_loop(self):\n",
    "        while True:\n",
    "            self.handle_player_input()\n",
    "            self.dqnDriver.step()\n",
    "                \n",
    "            self.UI.draw_hud()\n",
    "            self.UI.draw_blocks()\n",
    "            pygame.display.update()\n",
    "            self.clock.tick(TICK_RATE)\n",
    "\n",
    "    def handle_player_input(self):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                exit()\n",
    "\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_RETURN or event.key == pygame.K_SPACE:\n",
    "                    self.gameWorld.__init__()\n",
    "\n",
    "                elif event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04d8b787-b46b-49d1-9558-ced4a4c0662b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kin20\\.conda\\envs\\portfolio_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "myGame = Game(test)\n",
    "myGame.game_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ded9f12-84a1-4b74-9668-1451d94dd283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mreplay)):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(test\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mreplay[i][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mone_hot_to_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m     15\u001b[0m     state\u001b[38;5;241m.\u001b[39mappend((x,y))\n",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m, in \u001b[0;36mone_hot_to_coordinates\u001b[1;34m(one_hot)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# No non-zero elements found\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m indices\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [x, y]\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "def one_hot_to_coordinates(one_hot):\n",
    "    indices = np.argwhere(one_hot == 1)\n",
    "    if len(indices) == 0:\n",
    "        return None  # No non-zero elements found\n",
    "    else:\n",
    "        x, y = indices\n",
    "        return [x, y]\n",
    "\n",
    "state = []\n",
    "newState = []\n",
    "for i in range(len(test.agent.replay)):\n",
    "    print(test.agent.replay[i][0])\n",
    "    x = one_hot_to_coordinates(test.agent.replay[i][0][:24])\n",
    "    print(x)\n",
    "    state.append((x,y))\n",
    "    x, y = one_hot_to_coordinates(test.agent.replay[i][3][:24])\n",
    "    newState.append((x,y))\n",
    "action = test.agent.replay[1]\n",
    "reward = test.agent.replay[2]\n",
    "\n",
    "for i in range(len(state)):\n",
    "    ic(state, action, reward, newState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4c213-3909-49c3-ac83-eb8c1950fb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2960f44-f0d8-4152-827b-f975e8b7bf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
